{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 2970,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.010101010101010102,
      "grad_norm": 2.238417863845825,
      "learning_rate": 4.984848484848485e-05,
      "loss": 2.3653,
      "step": 10
    },
    {
      "epoch": 0.020202020202020204,
      "grad_norm": 3.785888910293579,
      "learning_rate": 4.9680134680134685e-05,
      "loss": 2.2138,
      "step": 20
    },
    {
      "epoch": 0.030303030303030304,
      "grad_norm": 5.4021220207214355,
      "learning_rate": 4.9511784511784515e-05,
      "loss": 1.9484,
      "step": 30
    },
    {
      "epoch": 0.04040404040404041,
      "grad_norm": 5.051417350769043,
      "learning_rate": 4.9343434343434346e-05,
      "loss": 1.5928,
      "step": 40
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 5.016143798828125,
      "learning_rate": 4.917508417508418e-05,
      "loss": 1.3023,
      "step": 50
    },
    {
      "epoch": 0.06060606060606061,
      "grad_norm": 6.082554817199707,
      "learning_rate": 4.9006734006734006e-05,
      "loss": 1.0876,
      "step": 60
    },
    {
      "epoch": 0.0707070707070707,
      "grad_norm": 4.727804183959961,
      "learning_rate": 4.8838383838383836e-05,
      "loss": 0.7972,
      "step": 70
    },
    {
      "epoch": 0.08080808080808081,
      "grad_norm": 6.484406471252441,
      "learning_rate": 4.867003367003367e-05,
      "loss": 0.7087,
      "step": 80
    },
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 4.726645469665527,
      "learning_rate": 4.8501683501683503e-05,
      "loss": 0.5411,
      "step": 90
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 10.246587753295898,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 0.4661,
      "step": 100
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 2.211913824081421,
      "learning_rate": 4.816498316498317e-05,
      "loss": 0.3249,
      "step": 110
    },
    {
      "epoch": 0.12121212121212122,
      "grad_norm": 8.138470649719238,
      "learning_rate": 4.7996632996633e-05,
      "loss": 0.2833,
      "step": 120
    },
    {
      "epoch": 0.13131313131313133,
      "grad_norm": 0.762740433216095,
      "learning_rate": 4.782828282828283e-05,
      "loss": 0.2309,
      "step": 130
    },
    {
      "epoch": 0.1414141414141414,
      "grad_norm": 2.6266040802001953,
      "learning_rate": 4.765993265993266e-05,
      "loss": 0.224,
      "step": 140
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 11.119760513305664,
      "learning_rate": 4.749158249158249e-05,
      "loss": 0.2969,
      "step": 150
    },
    {
      "epoch": 0.16161616161616163,
      "grad_norm": 7.659603118896484,
      "learning_rate": 4.732323232323232e-05,
      "loss": 0.2185,
      "step": 160
    },
    {
      "epoch": 0.1717171717171717,
      "grad_norm": 0.4306927025318146,
      "learning_rate": 4.715488215488216e-05,
      "loss": 0.2207,
      "step": 170
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 5.481110095977783,
      "learning_rate": 4.698653198653199e-05,
      "loss": 0.3477,
      "step": 180
    },
    {
      "epoch": 0.1919191919191919,
      "grad_norm": 0.3129503130912781,
      "learning_rate": 4.681818181818182e-05,
      "loss": 0.3394,
      "step": 190
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 8.074684143066406,
      "learning_rate": 4.6649831649831656e-05,
      "loss": 0.174,
      "step": 200
    },
    {
      "epoch": 0.21212121212121213,
      "grad_norm": 9.167783737182617,
      "learning_rate": 4.648148148148148e-05,
      "loss": 0.1462,
      "step": 210
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.6784790754318237,
      "learning_rate": 4.6313131313131316e-05,
      "loss": 0.2258,
      "step": 220
    },
    {
      "epoch": 0.23232323232323232,
      "grad_norm": 11.170684814453125,
      "learning_rate": 4.6144781144781147e-05,
      "loss": 0.1436,
      "step": 230
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 6.567673683166504,
      "learning_rate": 4.597643097643098e-05,
      "loss": 0.2,
      "step": 240
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 12.347599029541016,
      "learning_rate": 4.5808080808080814e-05,
      "loss": 0.2744,
      "step": 250
    },
    {
      "epoch": 0.26262626262626265,
      "grad_norm": 0.5729760527610779,
      "learning_rate": 4.5639730639730644e-05,
      "loss": 0.1825,
      "step": 260
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 17.049907684326172,
      "learning_rate": 4.5471380471380474e-05,
      "loss": 0.1843,
      "step": 270
    },
    {
      "epoch": 0.2828282828282828,
      "grad_norm": 4.097984313964844,
      "learning_rate": 4.5303030303030304e-05,
      "loss": 0.0953,
      "step": 280
    },
    {
      "epoch": 0.29292929292929293,
      "grad_norm": 7.080068588256836,
      "learning_rate": 4.5134680134680135e-05,
      "loss": 0.0721,
      "step": 290
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 29.446964263916016,
      "learning_rate": 4.4966329966329965e-05,
      "loss": 0.161,
      "step": 300
    },
    {
      "epoch": 0.31313131313131315,
      "grad_norm": 0.7431248426437378,
      "learning_rate": 4.47979797979798e-05,
      "loss": 0.1162,
      "step": 310
    },
    {
      "epoch": 0.32323232323232326,
      "grad_norm": 7.011728763580322,
      "learning_rate": 4.462962962962963e-05,
      "loss": 0.1677,
      "step": 320
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 0.0858258605003357,
      "learning_rate": 4.446127946127946e-05,
      "loss": 0.0596,
      "step": 330
    },
    {
      "epoch": 0.3434343434343434,
      "grad_norm": 0.11727453768253326,
      "learning_rate": 4.42929292929293e-05,
      "loss": 0.1232,
      "step": 340
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 9.651119232177734,
      "learning_rate": 4.412457912457912e-05,
      "loss": 0.0623,
      "step": 350
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.946788489818573,
      "learning_rate": 4.395622895622896e-05,
      "loss": 0.0325,
      "step": 360
    },
    {
      "epoch": 0.37373737373737376,
      "grad_norm": 9.069499015808105,
      "learning_rate": 4.378787878787879e-05,
      "loss": 0.1236,
      "step": 370
    },
    {
      "epoch": 0.3838383838383838,
      "grad_norm": 4.710905075073242,
      "learning_rate": 4.361952861952862e-05,
      "loss": 0.0846,
      "step": 380
    },
    {
      "epoch": 0.3939393939393939,
      "grad_norm": 15.028176307678223,
      "learning_rate": 4.345117845117846e-05,
      "loss": 0.1909,
      "step": 390
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 0.09941628575325012,
      "learning_rate": 4.328282828282829e-05,
      "loss": 0.1045,
      "step": 400
    },
    {
      "epoch": 0.41414141414141414,
      "grad_norm": 0.3459659814834595,
      "learning_rate": 4.311447811447812e-05,
      "loss": 0.0602,
      "step": 410
    },
    {
      "epoch": 0.42424242424242425,
      "grad_norm": 5.959925174713135,
      "learning_rate": 4.294612794612795e-05,
      "loss": 0.1921,
      "step": 420
    },
    {
      "epoch": 0.43434343434343436,
      "grad_norm": 32.22338104248047,
      "learning_rate": 4.277777777777778e-05,
      "loss": 0.1599,
      "step": 430
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 4.0331573486328125,
      "learning_rate": 4.260942760942761e-05,
      "loss": 0.1997,
      "step": 440
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 4.607197284698486,
      "learning_rate": 4.2441077441077445e-05,
      "loss": 0.1436,
      "step": 450
    },
    {
      "epoch": 0.46464646464646464,
      "grad_norm": 4.002233028411865,
      "learning_rate": 4.2272727272727275e-05,
      "loss": 0.1216,
      "step": 460
    },
    {
      "epoch": 0.47474747474747475,
      "grad_norm": 0.3895792067050934,
      "learning_rate": 4.2104377104377105e-05,
      "loss": 0.1786,
      "step": 470
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 1.5055749416351318,
      "learning_rate": 4.193602693602694e-05,
      "loss": 0.0831,
      "step": 480
    },
    {
      "epoch": 0.494949494949495,
      "grad_norm": 0.062405455857515335,
      "learning_rate": 4.176767676767677e-05,
      "loss": 0.1492,
      "step": 490
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 0.3682164251804352,
      "learning_rate": 4.1599326599326596e-05,
      "loss": 0.2289,
      "step": 500
    },
    {
      "epoch": 0.5151515151515151,
      "grad_norm": 13.128190994262695,
      "learning_rate": 4.143097643097643e-05,
      "loss": 0.129,
      "step": 510
    },
    {
      "epoch": 0.5252525252525253,
      "grad_norm": 10.361889839172363,
      "learning_rate": 4.126262626262626e-05,
      "loss": 0.1634,
      "step": 520
    },
    {
      "epoch": 0.5353535353535354,
      "grad_norm": 1.5324949026107788,
      "learning_rate": 4.1094276094276093e-05,
      "loss": 0.0446,
      "step": 530
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 1.6741865873336792,
      "learning_rate": 4.092592592592593e-05,
      "loss": 0.0662,
      "step": 540
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 6.919693470001221,
      "learning_rate": 4.075757575757576e-05,
      "loss": 0.1283,
      "step": 550
    },
    {
      "epoch": 0.5656565656565656,
      "grad_norm": 20.517168045043945,
      "learning_rate": 4.058922558922559e-05,
      "loss": 0.1695,
      "step": 560
    },
    {
      "epoch": 0.5757575757575758,
      "grad_norm": 2.1699509620666504,
      "learning_rate": 4.042087542087542e-05,
      "loss": 0.0978,
      "step": 570
    },
    {
      "epoch": 0.5858585858585859,
      "grad_norm": 0.3501414656639099,
      "learning_rate": 4.025252525252525e-05,
      "loss": 0.1049,
      "step": 580
    },
    {
      "epoch": 0.5959595959595959,
      "grad_norm": 0.07604911923408508,
      "learning_rate": 4.008417508417509e-05,
      "loss": 0.0885,
      "step": 590
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 0.02839699015021324,
      "learning_rate": 3.991582491582492e-05,
      "loss": 0.0575,
      "step": 600
    },
    {
      "epoch": 0.6161616161616161,
      "grad_norm": 10.317880630493164,
      "learning_rate": 3.974747474747475e-05,
      "loss": 0.1794,
      "step": 610
    },
    {
      "epoch": 0.6262626262626263,
      "grad_norm": 4.349177360534668,
      "learning_rate": 3.9579124579124586e-05,
      "loss": 0.1904,
      "step": 620
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 14.930983543395996,
      "learning_rate": 3.9410774410774416e-05,
      "loss": 0.0448,
      "step": 630
    },
    {
      "epoch": 0.6464646464646465,
      "grad_norm": 11.682177543640137,
      "learning_rate": 3.924242424242424e-05,
      "loss": 0.0808,
      "step": 640
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 13.509270668029785,
      "learning_rate": 3.9074074074074076e-05,
      "loss": 0.0565,
      "step": 650
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.018650956451892853,
      "learning_rate": 3.8905723905723906e-05,
      "loss": 0.0756,
      "step": 660
    },
    {
      "epoch": 0.6767676767676768,
      "grad_norm": 0.3003370761871338,
      "learning_rate": 3.8737373737373737e-05,
      "loss": 0.0277,
      "step": 670
    },
    {
      "epoch": 0.6868686868686869,
      "grad_norm": 0.1736309826374054,
      "learning_rate": 3.8569023569023574e-05,
      "loss": 0.0293,
      "step": 680
    },
    {
      "epoch": 0.696969696969697,
      "grad_norm": 0.027784643694758415,
      "learning_rate": 3.8400673400673404e-05,
      "loss": 0.0816,
      "step": 690
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 0.48066359758377075,
      "learning_rate": 3.8232323232323234e-05,
      "loss": 0.1029,
      "step": 700
    },
    {
      "epoch": 0.7171717171717171,
      "grad_norm": 0.15555627644062042,
      "learning_rate": 3.806397306397307e-05,
      "loss": 0.0609,
      "step": 710
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 4.144461631774902,
      "learning_rate": 3.7895622895622894e-05,
      "loss": 0.0708,
      "step": 720
    },
    {
      "epoch": 0.7373737373737373,
      "grad_norm": 22.19833755493164,
      "learning_rate": 3.7727272727272725e-05,
      "loss": 0.1323,
      "step": 730
    },
    {
      "epoch": 0.7474747474747475,
      "grad_norm": 10.432446479797363,
      "learning_rate": 3.755892255892256e-05,
      "loss": 0.1734,
      "step": 740
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 0.04320012778043747,
      "learning_rate": 3.739057239057239e-05,
      "loss": 0.1272,
      "step": 750
    },
    {
      "epoch": 0.7676767676767676,
      "grad_norm": 6.762394428253174,
      "learning_rate": 3.722222222222222e-05,
      "loss": 0.2917,
      "step": 760
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.40832406282424927,
      "learning_rate": 3.705387205387206e-05,
      "loss": 0.1,
      "step": 770
    },
    {
      "epoch": 0.7878787878787878,
      "grad_norm": 12.598819732666016,
      "learning_rate": 3.688552188552189e-05,
      "loss": 0.1522,
      "step": 780
    },
    {
      "epoch": 0.797979797979798,
      "grad_norm": 7.919632434844971,
      "learning_rate": 3.671717171717172e-05,
      "loss": 0.1498,
      "step": 790
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 0.24125172197818756,
      "learning_rate": 3.654882154882155e-05,
      "loss": 0.2709,
      "step": 800
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 0.7208662629127502,
      "learning_rate": 3.638047138047138e-05,
      "loss": 0.0789,
      "step": 810
    },
    {
      "epoch": 0.8282828282828283,
      "grad_norm": 11.329678535461426,
      "learning_rate": 3.621212121212122e-05,
      "loss": 0.1296,
      "step": 820
    },
    {
      "epoch": 0.8383838383838383,
      "grad_norm": 13.093029975891113,
      "learning_rate": 3.604377104377105e-05,
      "loss": 0.2032,
      "step": 830
    },
    {
      "epoch": 0.8484848484848485,
      "grad_norm": 1.5768824815750122,
      "learning_rate": 3.587542087542088e-05,
      "loss": 0.1546,
      "step": 840
    },
    {
      "epoch": 0.8585858585858586,
      "grad_norm": 3.4023430347442627,
      "learning_rate": 3.5707070707070714e-05,
      "loss": 0.2156,
      "step": 850
    },
    {
      "epoch": 0.8686868686868687,
      "grad_norm": 0.08072983473539352,
      "learning_rate": 3.553872053872054e-05,
      "loss": 0.111,
      "step": 860
    },
    {
      "epoch": 0.8787878787878788,
      "grad_norm": 3.17338228225708,
      "learning_rate": 3.537037037037037e-05,
      "loss": 0.2386,
      "step": 870
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.050061535090208054,
      "learning_rate": 3.5202020202020205e-05,
      "loss": 0.0672,
      "step": 880
    },
    {
      "epoch": 0.898989898989899,
      "grad_norm": 5.3977370262146,
      "learning_rate": 3.5033670033670035e-05,
      "loss": 0.07,
      "step": 890
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 9.538756370544434,
      "learning_rate": 3.4865319865319865e-05,
      "loss": 0.1766,
      "step": 900
    },
    {
      "epoch": 0.9191919191919192,
      "grad_norm": 6.984779357910156,
      "learning_rate": 3.46969696969697e-05,
      "loss": 0.1081,
      "step": 910
    },
    {
      "epoch": 0.9292929292929293,
      "grad_norm": 19.801393508911133,
      "learning_rate": 3.452861952861953e-05,
      "loss": 0.1314,
      "step": 920
    },
    {
      "epoch": 0.9393939393939394,
      "grad_norm": 0.8525626063346863,
      "learning_rate": 3.436026936026936e-05,
      "loss": 0.1184,
      "step": 930
    },
    {
      "epoch": 0.9494949494949495,
      "grad_norm": 0.278260737657547,
      "learning_rate": 3.419191919191919e-05,
      "loss": 0.0441,
      "step": 940
    },
    {
      "epoch": 0.9595959595959596,
      "grad_norm": 0.02758600190281868,
      "learning_rate": 3.402356902356902e-05,
      "loss": 0.0736,
      "step": 950
    },
    {
      "epoch": 0.9696969696969697,
      "grad_norm": 0.7413772344589233,
      "learning_rate": 3.385521885521885e-05,
      "loss": 0.1002,
      "step": 960
    },
    {
      "epoch": 0.9797979797979798,
      "grad_norm": 0.045104287564754486,
      "learning_rate": 3.368686868686869e-05,
      "loss": 0.1215,
      "step": 970
    },
    {
      "epoch": 0.98989898989899,
      "grad_norm": 2.7119088172912598,
      "learning_rate": 3.351851851851852e-05,
      "loss": 0.0616,
      "step": 980
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.03202301263809204,
      "learning_rate": 3.335016835016835e-05,
      "loss": 0.1504,
      "step": 990
    },
    {
      "epoch": 1.0101010101010102,
      "grad_norm": 0.10132811963558197,
      "learning_rate": 3.318181818181819e-05,
      "loss": 0.0732,
      "step": 1000
    },
    {
      "epoch": 1.02020202020202,
      "grad_norm": 0.28551939129829407,
      "learning_rate": 3.301346801346801e-05,
      "loss": 0.014,
      "step": 1010
    },
    {
      "epoch": 1.0303030303030303,
      "grad_norm": 0.040963202714920044,
      "learning_rate": 3.284511784511785e-05,
      "loss": 0.0284,
      "step": 1020
    },
    {
      "epoch": 1.0404040404040404,
      "grad_norm": 10.426454544067383,
      "learning_rate": 3.267676767676768e-05,
      "loss": 0.1113,
      "step": 1030
    },
    {
      "epoch": 1.0505050505050506,
      "grad_norm": 0.2042720913887024,
      "learning_rate": 3.250841750841751e-05,
      "loss": 0.1042,
      "step": 1040
    },
    {
      "epoch": 1.0606060606060606,
      "grad_norm": 0.15798626840114594,
      "learning_rate": 3.2340067340067345e-05,
      "loss": 0.0026,
      "step": 1050
    },
    {
      "epoch": 1.0707070707070707,
      "grad_norm": 0.022041575983166695,
      "learning_rate": 3.2171717171717176e-05,
      "loss": 0.0718,
      "step": 1060
    },
    {
      "epoch": 1.0808080808080809,
      "grad_norm": 0.06596854329109192,
      "learning_rate": 3.2003367003367006e-05,
      "loss": 0.0251,
      "step": 1070
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.019285866990685463,
      "learning_rate": 3.1835016835016836e-05,
      "loss": 0.0498,
      "step": 1080
    },
    {
      "epoch": 1.101010101010101,
      "grad_norm": 0.01787417382001877,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 0.0088,
      "step": 1090
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.023844130337238312,
      "learning_rate": 3.1498316498316496e-05,
      "loss": 0.0098,
      "step": 1100
    },
    {
      "epoch": 1.121212121212121,
      "grad_norm": 0.08855897188186646,
      "learning_rate": 3.132996632996633e-05,
      "loss": 0.0251,
      "step": 1110
    },
    {
      "epoch": 1.1313131313131313,
      "grad_norm": 0.05508262291550636,
      "learning_rate": 3.1161616161616164e-05,
      "loss": 0.0406,
      "step": 1120
    },
    {
      "epoch": 1.1414141414141414,
      "grad_norm": 49.81795883178711,
      "learning_rate": 3.0993265993265994e-05,
      "loss": 0.0217,
      "step": 1130
    },
    {
      "epoch": 1.1515151515151516,
      "grad_norm": 0.06203470751643181,
      "learning_rate": 3.082491582491583e-05,
      "loss": 0.0401,
      "step": 1140
    },
    {
      "epoch": 1.1616161616161615,
      "grad_norm": 0.011384954676032066,
      "learning_rate": 3.0656565656565654e-05,
      "loss": 0.0313,
      "step": 1150
    },
    {
      "epoch": 1.1717171717171717,
      "grad_norm": 0.02075321413576603,
      "learning_rate": 3.0488215488215488e-05,
      "loss": 0.0646,
      "step": 1160
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 0.016396310180425644,
      "learning_rate": 3.031986531986532e-05,
      "loss": 0.0233,
      "step": 1170
    },
    {
      "epoch": 1.1919191919191918,
      "grad_norm": 0.10310479998588562,
      "learning_rate": 3.015151515151515e-05,
      "loss": 0.0045,
      "step": 1180
    },
    {
      "epoch": 1.202020202020202,
      "grad_norm": 0.011378764174878597,
      "learning_rate": 2.9983164983164985e-05,
      "loss": 0.0041,
      "step": 1190
    },
    {
      "epoch": 1.2121212121212122,
      "grad_norm": 0.017177117988467216,
      "learning_rate": 2.981481481481482e-05,
      "loss": 0.1293,
      "step": 1200
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 0.01172136515378952,
      "learning_rate": 2.964646464646465e-05,
      "loss": 0.0375,
      "step": 1210
    },
    {
      "epoch": 1.2323232323232323,
      "grad_norm": 8.709439277648926,
      "learning_rate": 2.9478114478114476e-05,
      "loss": 0.0653,
      "step": 1220
    },
    {
      "epoch": 1.2424242424242424,
      "grad_norm": 0.043131273239851,
      "learning_rate": 2.930976430976431e-05,
      "loss": 0.0154,
      "step": 1230
    },
    {
      "epoch": 1.2525252525252526,
      "grad_norm": 10.978713989257812,
      "learning_rate": 2.9141414141414143e-05,
      "loss": 0.0941,
      "step": 1240
    },
    {
      "epoch": 1.2626262626262625,
      "grad_norm": 0.01637129671871662,
      "learning_rate": 2.8973063973063973e-05,
      "loss": 0.032,
      "step": 1250
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.018860597163438797,
      "learning_rate": 2.8804713804713807e-05,
      "loss": 0.0031,
      "step": 1260
    },
    {
      "epoch": 1.2828282828282829,
      "grad_norm": 14.113204956054688,
      "learning_rate": 2.863636363636364e-05,
      "loss": 0.1101,
      "step": 1270
    },
    {
      "epoch": 1.2929292929292928,
      "grad_norm": 13.608972549438477,
      "learning_rate": 2.846801346801347e-05,
      "loss": 0.1512,
      "step": 1280
    },
    {
      "epoch": 1.303030303030303,
      "grad_norm": 0.010722481645643711,
      "learning_rate": 2.8299663299663304e-05,
      "loss": 0.0218,
      "step": 1290
    },
    {
      "epoch": 1.3131313131313131,
      "grad_norm": 0.02212299406528473,
      "learning_rate": 2.813131313131313e-05,
      "loss": 0.1464,
      "step": 1300
    },
    {
      "epoch": 1.3232323232323233,
      "grad_norm": 0.02163929119706154,
      "learning_rate": 2.7962962962962965e-05,
      "loss": 0.0968,
      "step": 1310
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.1699167937040329,
      "learning_rate": 2.7794612794612795e-05,
      "loss": 0.0134,
      "step": 1320
    },
    {
      "epoch": 1.3434343434343434,
      "grad_norm": 0.011663897894322872,
      "learning_rate": 2.762626262626263e-05,
      "loss": 0.0079,
      "step": 1330
    },
    {
      "epoch": 1.3535353535353536,
      "grad_norm": 0.008350498974323273,
      "learning_rate": 2.7457912457912462e-05,
      "loss": 0.0215,
      "step": 1340
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 0.309698224067688,
      "learning_rate": 2.7289562289562292e-05,
      "loss": 0.067,
      "step": 1350
    },
    {
      "epoch": 1.3737373737373737,
      "grad_norm": 8.25751781463623,
      "learning_rate": 2.7121212121212126e-05,
      "loss": 0.0609,
      "step": 1360
    },
    {
      "epoch": 1.3838383838383839,
      "grad_norm": 0.0108980443328619,
      "learning_rate": 2.6952861952861953e-05,
      "loss": 0.0035,
      "step": 1370
    },
    {
      "epoch": 1.393939393939394,
      "grad_norm": 0.00893368013203144,
      "learning_rate": 2.6784511784511783e-05,
      "loss": 0.0688,
      "step": 1380
    },
    {
      "epoch": 1.404040404040404,
      "grad_norm": 0.010779902338981628,
      "learning_rate": 2.6616161616161616e-05,
      "loss": 0.0836,
      "step": 1390
    },
    {
      "epoch": 1.4141414141414141,
      "grad_norm": 0.06257230788469315,
      "learning_rate": 2.644781144781145e-05,
      "loss": 0.046,
      "step": 1400
    },
    {
      "epoch": 1.4242424242424243,
      "grad_norm": 0.012892195023596287,
      "learning_rate": 2.627946127946128e-05,
      "loss": 0.0316,
      "step": 1410
    },
    {
      "epoch": 1.4343434343434343,
      "grad_norm": 0.7493061423301697,
      "learning_rate": 2.6111111111111114e-05,
      "loss": 0.0343,
      "step": 1420
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.04511284455657005,
      "learning_rate": 2.5942760942760947e-05,
      "loss": 0.0049,
      "step": 1430
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.027387652546167374,
      "learning_rate": 2.5774410774410774e-05,
      "loss": 0.0077,
      "step": 1440
    },
    {
      "epoch": 1.4646464646464645,
      "grad_norm": 0.020449819043278694,
      "learning_rate": 2.5606060606060604e-05,
      "loss": 0.0049,
      "step": 1450
    },
    {
      "epoch": 1.4747474747474747,
      "grad_norm": 0.011215039528906345,
      "learning_rate": 2.5437710437710438e-05,
      "loss": 0.0078,
      "step": 1460
    },
    {
      "epoch": 1.4848484848484849,
      "grad_norm": 0.010359128005802631,
      "learning_rate": 2.526936026936027e-05,
      "loss": 0.008,
      "step": 1470
    },
    {
      "epoch": 1.494949494949495,
      "grad_norm": 0.017538517713546753,
      "learning_rate": 2.5101010101010102e-05,
      "loss": 0.0256,
      "step": 1480
    },
    {
      "epoch": 1.5050505050505052,
      "grad_norm": 24.973413467407227,
      "learning_rate": 2.4932659932659932e-05,
      "loss": 0.0485,
      "step": 1490
    },
    {
      "epoch": 1.5151515151515151,
      "grad_norm": 3.3446555137634277,
      "learning_rate": 2.4764309764309766e-05,
      "loss": 0.0682,
      "step": 1500
    },
    {
      "epoch": 1.5252525252525253,
      "grad_norm": 0.007420492358505726,
      "learning_rate": 2.4595959595959596e-05,
      "loss": 0.001,
      "step": 1510
    },
    {
      "epoch": 1.5353535353535355,
      "grad_norm": 0.07076955586671829,
      "learning_rate": 2.442760942760943e-05,
      "loss": 0.0147,
      "step": 1520
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 0.010019480250775814,
      "learning_rate": 2.425925925925926e-05,
      "loss": 0.0063,
      "step": 1530
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 8.640168190002441,
      "learning_rate": 2.4090909090909093e-05,
      "loss": 0.0277,
      "step": 1540
    },
    {
      "epoch": 1.5656565656565657,
      "grad_norm": 7.456233978271484,
      "learning_rate": 2.3922558922558923e-05,
      "loss": 0.0549,
      "step": 1550
    },
    {
      "epoch": 1.5757575757575757,
      "grad_norm": 0.007029993459582329,
      "learning_rate": 2.3754208754208757e-05,
      "loss": 0.0624,
      "step": 1560
    },
    {
      "epoch": 1.5858585858585859,
      "grad_norm": 0.024093806743621826,
      "learning_rate": 2.3585858585858587e-05,
      "loss": 0.0013,
      "step": 1570
    },
    {
      "epoch": 1.595959595959596,
      "grad_norm": 2.7335405349731445,
      "learning_rate": 2.3417508417508417e-05,
      "loss": 0.0952,
      "step": 1580
    },
    {
      "epoch": 1.606060606060606,
      "grad_norm": 0.020306739956140518,
      "learning_rate": 2.324915824915825e-05,
      "loss": 0.0009,
      "step": 1590
    },
    {
      "epoch": 1.6161616161616161,
      "grad_norm": 0.01460237242281437,
      "learning_rate": 2.308080808080808e-05,
      "loss": 0.031,
      "step": 1600
    },
    {
      "epoch": 1.6262626262626263,
      "grad_norm": 0.007364913821220398,
      "learning_rate": 2.2912457912457915e-05,
      "loss": 0.0593,
      "step": 1610
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.036033932119607925,
      "learning_rate": 2.2744107744107745e-05,
      "loss": 0.0851,
      "step": 1620
    },
    {
      "epoch": 1.6464646464646466,
      "grad_norm": 0.047457098960876465,
      "learning_rate": 2.257575757575758e-05,
      "loss": 0.0421,
      "step": 1630
    },
    {
      "epoch": 1.6565656565656566,
      "grad_norm": 0.006878308020532131,
      "learning_rate": 2.240740740740741e-05,
      "loss": 0.1098,
      "step": 1640
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.012278567999601364,
      "learning_rate": 2.223905723905724e-05,
      "loss": 0.0049,
      "step": 1650
    },
    {
      "epoch": 1.676767676767677,
      "grad_norm": 0.009618713520467281,
      "learning_rate": 2.2070707070707073e-05,
      "loss": 0.0533,
      "step": 1660
    },
    {
      "epoch": 1.6868686868686869,
      "grad_norm": 0.007546384818851948,
      "learning_rate": 2.1902356902356906e-05,
      "loss": 0.03,
      "step": 1670
    },
    {
      "epoch": 1.696969696969697,
      "grad_norm": 1.0792843103408813,
      "learning_rate": 2.1734006734006733e-05,
      "loss": 0.0135,
      "step": 1680
    },
    {
      "epoch": 1.7070707070707072,
      "grad_norm": 13.099847793579102,
      "learning_rate": 2.1565656565656567e-05,
      "loss": 0.0227,
      "step": 1690
    },
    {
      "epoch": 1.7171717171717171,
      "grad_norm": 0.017438096925616264,
      "learning_rate": 2.13973063973064e-05,
      "loss": 0.0065,
      "step": 1700
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 0.012722809799015522,
      "learning_rate": 2.122895622895623e-05,
      "loss": 0.0554,
      "step": 1710
    },
    {
      "epoch": 1.7373737373737375,
      "grad_norm": 0.006778932176530361,
      "learning_rate": 2.106060606060606e-05,
      "loss": 0.0082,
      "step": 1720
    },
    {
      "epoch": 1.7474747474747474,
      "grad_norm": 21.678525924682617,
      "learning_rate": 2.0892255892255894e-05,
      "loss": 0.0525,
      "step": 1730
    },
    {
      "epoch": 1.7575757575757576,
      "grad_norm": 0.005549629218876362,
      "learning_rate": 2.0723905723905728e-05,
      "loss": 0.0015,
      "step": 1740
    },
    {
      "epoch": 1.7676767676767677,
      "grad_norm": 0.013625700026750565,
      "learning_rate": 2.0555555555555555e-05,
      "loss": 0.0214,
      "step": 1750
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 0.01905926689505577,
      "learning_rate": 2.0387205387205388e-05,
      "loss": 0.0172,
      "step": 1760
    },
    {
      "epoch": 1.7878787878787878,
      "grad_norm": 0.012057178653776646,
      "learning_rate": 2.021885521885522e-05,
      "loss": 0.0058,
      "step": 1770
    },
    {
      "epoch": 1.797979797979798,
      "grad_norm": 22.91185760498047,
      "learning_rate": 2.005050505050505e-05,
      "loss": 0.0377,
      "step": 1780
    },
    {
      "epoch": 1.808080808080808,
      "grad_norm": 0.18693406879901886,
      "learning_rate": 1.9882154882154882e-05,
      "loss": 0.0381,
      "step": 1790
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.2740100622177124,
      "learning_rate": 1.9713804713804716e-05,
      "loss": 0.0794,
      "step": 1800
    },
    {
      "epoch": 1.8282828282828283,
      "grad_norm": 0.004856002517044544,
      "learning_rate": 1.9545454545454546e-05,
      "loss": 0.1218,
      "step": 1810
    },
    {
      "epoch": 1.8383838383838382,
      "grad_norm": 0.15397755801677704,
      "learning_rate": 1.9377104377104376e-05,
      "loss": 0.0046,
      "step": 1820
    },
    {
      "epoch": 1.8484848484848486,
      "grad_norm": 0.1291525959968567,
      "learning_rate": 1.920875420875421e-05,
      "loss": 0.0584,
      "step": 1830
    },
    {
      "epoch": 1.8585858585858586,
      "grad_norm": 15.137090682983398,
      "learning_rate": 1.9040404040404043e-05,
      "loss": 0.0269,
      "step": 1840
    },
    {
      "epoch": 1.8686868686868687,
      "grad_norm": 0.016789516434073448,
      "learning_rate": 1.8872053872053873e-05,
      "loss": 0.07,
      "step": 1850
    },
    {
      "epoch": 1.878787878787879,
      "grad_norm": 0.012824192643165588,
      "learning_rate": 1.8703703703703704e-05,
      "loss": 0.0839,
      "step": 1860
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.014176907949149609,
      "learning_rate": 1.8535353535353537e-05,
      "loss": 0.0706,
      "step": 1870
    },
    {
      "epoch": 1.898989898989899,
      "grad_norm": 0.050623372197151184,
      "learning_rate": 1.8367003367003367e-05,
      "loss": 0.0202,
      "step": 1880
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 0.012549984268844128,
      "learning_rate": 1.8198653198653198e-05,
      "loss": 0.0062,
      "step": 1890
    },
    {
      "epoch": 1.9191919191919191,
      "grad_norm": 0.019345493987202644,
      "learning_rate": 1.803030303030303e-05,
      "loss": 0.0803,
      "step": 1900
    },
    {
      "epoch": 1.9292929292929293,
      "grad_norm": 0.0072960928082466125,
      "learning_rate": 1.786195286195286e-05,
      "loss": 0.0436,
      "step": 1910
    },
    {
      "epoch": 1.9393939393939394,
      "grad_norm": 0.01603221520781517,
      "learning_rate": 1.7693602693602695e-05,
      "loss": 0.0378,
      "step": 1920
    },
    {
      "epoch": 1.9494949494949494,
      "grad_norm": 0.0071825855411589146,
      "learning_rate": 1.7525252525252525e-05,
      "loss": 0.0567,
      "step": 1930
    },
    {
      "epoch": 1.9595959595959596,
      "grad_norm": 0.008669188246130943,
      "learning_rate": 1.735690235690236e-05,
      "loss": 0.04,
      "step": 1940
    },
    {
      "epoch": 1.9696969696969697,
      "grad_norm": 0.007374608423560858,
      "learning_rate": 1.718855218855219e-05,
      "loss": 0.0138,
      "step": 1950
    },
    {
      "epoch": 1.9797979797979797,
      "grad_norm": 0.02772117219865322,
      "learning_rate": 1.7020202020202023e-05,
      "loss": 0.0089,
      "step": 1960
    },
    {
      "epoch": 1.98989898989899,
      "grad_norm": 0.033521782606840134,
      "learning_rate": 1.6851851851851853e-05,
      "loss": 0.055,
      "step": 1970
    },
    {
      "epoch": 2.0,
      "grad_norm": 14.61590576171875,
      "learning_rate": 1.6683501683501683e-05,
      "loss": 0.0129,
      "step": 1980
    },
    {
      "epoch": 2.01010101010101,
      "grad_norm": 0.281324177980423,
      "learning_rate": 1.6515151515151517e-05,
      "loss": 0.0447,
      "step": 1990
    },
    {
      "epoch": 2.0202020202020203,
      "grad_norm": 0.01008414477109909,
      "learning_rate": 1.6346801346801347e-05,
      "loss": 0.0067,
      "step": 2000
    },
    {
      "epoch": 2.0303030303030303,
      "grad_norm": 0.0066777244210243225,
      "learning_rate": 1.617845117845118e-05,
      "loss": 0.0011,
      "step": 2010
    },
    {
      "epoch": 2.04040404040404,
      "grad_norm": 0.007474285084754229,
      "learning_rate": 1.601010101010101e-05,
      "loss": 0.0146,
      "step": 2020
    },
    {
      "epoch": 2.0505050505050506,
      "grad_norm": 2.274604320526123,
      "learning_rate": 1.5841750841750844e-05,
      "loss": 0.0037,
      "step": 2030
    },
    {
      "epoch": 2.0606060606060606,
      "grad_norm": 0.011437583714723587,
      "learning_rate": 1.5673400673400674e-05,
      "loss": 0.0375,
      "step": 2040
    },
    {
      "epoch": 2.0707070707070705,
      "grad_norm": 0.42196375131607056,
      "learning_rate": 1.5505050505050505e-05,
      "loss": 0.001,
      "step": 2050
    },
    {
      "epoch": 2.080808080808081,
      "grad_norm": 0.013225572183728218,
      "learning_rate": 1.5336700336700338e-05,
      "loss": 0.0489,
      "step": 2060
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 0.008439728058874607,
      "learning_rate": 1.5168350168350168e-05,
      "loss": 0.0007,
      "step": 2070
    },
    {
      "epoch": 2.101010101010101,
      "grad_norm": 0.008093624375760555,
      "learning_rate": 1.5e-05,
      "loss": 0.0006,
      "step": 2080
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 0.005045850295573473,
      "learning_rate": 1.4831649831649832e-05,
      "loss": 0.0036,
      "step": 2090
    },
    {
      "epoch": 2.121212121212121,
      "grad_norm": 2.831888198852539,
      "learning_rate": 1.4663299663299664e-05,
      "loss": 0.0537,
      "step": 2100
    },
    {
      "epoch": 2.1313131313131315,
      "grad_norm": 10.50881576538086,
      "learning_rate": 1.4494949494949494e-05,
      "loss": 0.0113,
      "step": 2110
    },
    {
      "epoch": 2.1414141414141414,
      "grad_norm": 0.12383566051721573,
      "learning_rate": 1.4326599326599326e-05,
      "loss": 0.0009,
      "step": 2120
    },
    {
      "epoch": 2.1515151515151514,
      "grad_norm": 0.01077314093708992,
      "learning_rate": 1.415824915824916e-05,
      "loss": 0.0009,
      "step": 2130
    },
    {
      "epoch": 2.1616161616161618,
      "grad_norm": 0.010086117312312126,
      "learning_rate": 1.3989898989898992e-05,
      "loss": 0.0248,
      "step": 2140
    },
    {
      "epoch": 2.1717171717171717,
      "grad_norm": 0.6471660137176514,
      "learning_rate": 1.3821548821548822e-05,
      "loss": 0.0012,
      "step": 2150
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.07888210564851761,
      "learning_rate": 1.3653198653198654e-05,
      "loss": 0.002,
      "step": 2160
    },
    {
      "epoch": 2.191919191919192,
      "grad_norm": 0.012021522969007492,
      "learning_rate": 1.3484848484848486e-05,
      "loss": 0.0108,
      "step": 2170
    },
    {
      "epoch": 2.202020202020202,
      "grad_norm": 0.009039260447025299,
      "learning_rate": 1.3316498316498316e-05,
      "loss": 0.0047,
      "step": 2180
    },
    {
      "epoch": 2.212121212121212,
      "grad_norm": 0.006143652833998203,
      "learning_rate": 1.3148148148148148e-05,
      "loss": 0.0007,
      "step": 2190
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 0.005134312901645899,
      "learning_rate": 1.2979797979797981e-05,
      "loss": 0.0017,
      "step": 2200
    },
    {
      "epoch": 2.2323232323232323,
      "grad_norm": 0.1445765495300293,
      "learning_rate": 1.2811447811447813e-05,
      "loss": 0.032,
      "step": 2210
    },
    {
      "epoch": 2.242424242424242,
      "grad_norm": 0.02517569251358509,
      "learning_rate": 1.2643097643097644e-05,
      "loss": 0.0006,
      "step": 2220
    },
    {
      "epoch": 2.2525252525252526,
      "grad_norm": 0.3303142786026001,
      "learning_rate": 1.2474747474747475e-05,
      "loss": 0.0438,
      "step": 2230
    },
    {
      "epoch": 2.2626262626262625,
      "grad_norm": 0.017680136486887932,
      "learning_rate": 1.2306397306397307e-05,
      "loss": 0.0012,
      "step": 2240
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 0.006375949364155531,
      "learning_rate": 1.213804713804714e-05,
      "loss": 0.0006,
      "step": 2250
    },
    {
      "epoch": 2.282828282828283,
      "grad_norm": 0.03375294804573059,
      "learning_rate": 1.196969696969697e-05,
      "loss": 0.0156,
      "step": 2260
    },
    {
      "epoch": 2.292929292929293,
      "grad_norm": 0.0057427952997386456,
      "learning_rate": 1.1801346801346801e-05,
      "loss": 0.0098,
      "step": 2270
    },
    {
      "epoch": 2.303030303030303,
      "grad_norm": 0.008838617242872715,
      "learning_rate": 1.1632996632996633e-05,
      "loss": 0.0006,
      "step": 2280
    },
    {
      "epoch": 2.313131313131313,
      "grad_norm": 0.014615370891988277,
      "learning_rate": 1.1464646464646465e-05,
      "loss": 0.0786,
      "step": 2290
    },
    {
      "epoch": 2.323232323232323,
      "grad_norm": 0.00423134071752429,
      "learning_rate": 1.1296296296296297e-05,
      "loss": 0.0008,
      "step": 2300
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.014948010444641113,
      "learning_rate": 1.1127946127946129e-05,
      "loss": 0.0259,
      "step": 2310
    },
    {
      "epoch": 2.3434343434343434,
      "grad_norm": 0.015090851113200188,
      "learning_rate": 1.095959595959596e-05,
      "loss": 0.0021,
      "step": 2320
    },
    {
      "epoch": 2.3535353535353534,
      "grad_norm": 0.020247800275683403,
      "learning_rate": 1.0791245791245793e-05,
      "loss": 0.0081,
      "step": 2330
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.006000608205795288,
      "learning_rate": 1.0622895622895623e-05,
      "loss": 0.0281,
      "step": 2340
    },
    {
      "epoch": 2.3737373737373737,
      "grad_norm": 0.0054956767708063126,
      "learning_rate": 1.0454545454545455e-05,
      "loss": 0.077,
      "step": 2350
    },
    {
      "epoch": 2.3838383838383836,
      "grad_norm": 16.003503799438477,
      "learning_rate": 1.0286195286195287e-05,
      "loss": 0.0208,
      "step": 2360
    },
    {
      "epoch": 2.393939393939394,
      "grad_norm": 0.022726677358150482,
      "learning_rate": 1.0117845117845117e-05,
      "loss": 0.0007,
      "step": 2370
    },
    {
      "epoch": 2.404040404040404,
      "grad_norm": 0.028106296434998512,
      "learning_rate": 9.94949494949495e-06,
      "loss": 0.0006,
      "step": 2380
    },
    {
      "epoch": 2.4141414141414144,
      "grad_norm": 0.0043794577941298485,
      "learning_rate": 9.78114478114478e-06,
      "loss": 0.0179,
      "step": 2390
    },
    {
      "epoch": 2.4242424242424243,
      "grad_norm": 0.005569914355874062,
      "learning_rate": 9.612794612794614e-06,
      "loss": 0.0021,
      "step": 2400
    },
    {
      "epoch": 2.4343434343434343,
      "grad_norm": 0.4326336085796356,
      "learning_rate": 9.444444444444445e-06,
      "loss": 0.0028,
      "step": 2410
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.0071491640992462635,
      "learning_rate": 9.276094276094276e-06,
      "loss": 0.0457,
      "step": 2420
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 0.0051436894573271275,
      "learning_rate": 9.107744107744108e-06,
      "loss": 0.0005,
      "step": 2430
    },
    {
      "epoch": 2.4646464646464645,
      "grad_norm": 0.00840364582836628,
      "learning_rate": 8.93939393939394e-06,
      "loss": 0.0008,
      "step": 2440
    },
    {
      "epoch": 2.474747474747475,
      "grad_norm": 0.005452763754874468,
      "learning_rate": 8.771043771043772e-06,
      "loss": 0.0006,
      "step": 2450
    },
    {
      "epoch": 2.484848484848485,
      "grad_norm": 11.280138969421387,
      "learning_rate": 8.602693602693602e-06,
      "loss": 0.0522,
      "step": 2460
    },
    {
      "epoch": 2.494949494949495,
      "grad_norm": 14.155250549316406,
      "learning_rate": 8.434343434343434e-06,
      "loss": 0.0052,
      "step": 2470
    },
    {
      "epoch": 2.505050505050505,
      "grad_norm": 9.6864652633667,
      "learning_rate": 8.265993265993266e-06,
      "loss": 0.0187,
      "step": 2480
    },
    {
      "epoch": 2.515151515151515,
      "grad_norm": 5.663662910461426,
      "learning_rate": 8.097643097643098e-06,
      "loss": 0.0087,
      "step": 2490
    },
    {
      "epoch": 2.525252525252525,
      "grad_norm": 0.006714435759931803,
      "learning_rate": 7.92929292929293e-06,
      "loss": 0.0008,
      "step": 2500
    },
    {
      "epoch": 2.5353535353535355,
      "grad_norm": 7.751681804656982,
      "learning_rate": 7.760942760942762e-06,
      "loss": 0.0083,
      "step": 2510
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.00789275299757719,
      "learning_rate": 7.592592592592593e-06,
      "loss": 0.0026,
      "step": 2520
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 0.00428697420284152,
      "learning_rate": 7.424242424242425e-06,
      "loss": 0.0005,
      "step": 2530
    },
    {
      "epoch": 2.5656565656565657,
      "grad_norm": 0.008541999384760857,
      "learning_rate": 7.255892255892256e-06,
      "loss": 0.0652,
      "step": 2540
    },
    {
      "epoch": 2.5757575757575757,
      "grad_norm": 0.0047448440454900265,
      "learning_rate": 7.087542087542087e-06,
      "loss": 0.0004,
      "step": 2550
    },
    {
      "epoch": 2.5858585858585856,
      "grad_norm": 12.507614135742188,
      "learning_rate": 6.91919191919192e-06,
      "loss": 0.0399,
      "step": 2560
    },
    {
      "epoch": 2.595959595959596,
      "grad_norm": 0.007212341763079166,
      "learning_rate": 6.750841750841751e-06,
      "loss": 0.0094,
      "step": 2570
    },
    {
      "epoch": 2.606060606060606,
      "grad_norm": 0.003389504970982671,
      "learning_rate": 6.5824915824915834e-06,
      "loss": 0.0182,
      "step": 2580
    },
    {
      "epoch": 2.616161616161616,
      "grad_norm": 0.003480068873614073,
      "learning_rate": 6.4141414141414145e-06,
      "loss": 0.0006,
      "step": 2590
    },
    {
      "epoch": 2.6262626262626263,
      "grad_norm": 0.004144617822021246,
      "learning_rate": 6.2457912457912455e-06,
      "loss": 0.0007,
      "step": 2600
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 0.004170003347098827,
      "learning_rate": 6.0774410774410774e-06,
      "loss": 0.0004,
      "step": 2610
    },
    {
      "epoch": 2.6464646464646466,
      "grad_norm": 0.005223214626312256,
      "learning_rate": 5.909090909090909e-06,
      "loss": 0.0019,
      "step": 2620
    },
    {
      "epoch": 2.6565656565656566,
      "grad_norm": 0.005804947577416897,
      "learning_rate": 5.740740740740741e-06,
      "loss": 0.0067,
      "step": 2630
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 0.8452252745628357,
      "learning_rate": 5.572390572390572e-06,
      "loss": 0.0019,
      "step": 2640
    },
    {
      "epoch": 2.676767676767677,
      "grad_norm": 0.0043539912439882755,
      "learning_rate": 5.404040404040404e-06,
      "loss": 0.0039,
      "step": 2650
    },
    {
      "epoch": 2.686868686868687,
      "grad_norm": 0.009022138081490993,
      "learning_rate": 5.235690235690236e-06,
      "loss": 0.0371,
      "step": 2660
    },
    {
      "epoch": 2.6969696969696972,
      "grad_norm": 1.024878740310669,
      "learning_rate": 5.067340067340068e-06,
      "loss": 0.0012,
      "step": 2670
    },
    {
      "epoch": 2.707070707070707,
      "grad_norm": 0.009439175948500633,
      "learning_rate": 4.8989898989899e-06,
      "loss": 0.0011,
      "step": 2680
    },
    {
      "epoch": 2.717171717171717,
      "grad_norm": 0.004208267200738192,
      "learning_rate": 4.730639730639731e-06,
      "loss": 0.0009,
      "step": 2690
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.004627983085811138,
      "learning_rate": 4.562289562289562e-06,
      "loss": 0.0084,
      "step": 2700
    },
    {
      "epoch": 2.7373737373737375,
      "grad_norm": 0.003545810468494892,
      "learning_rate": 4.393939393939394e-06,
      "loss": 0.0208,
      "step": 2710
    },
    {
      "epoch": 2.7474747474747474,
      "grad_norm": 0.004564001224935055,
      "learning_rate": 4.225589225589226e-06,
      "loss": 0.0255,
      "step": 2720
    },
    {
      "epoch": 2.757575757575758,
      "grad_norm": 0.005099640227854252,
      "learning_rate": 4.057239057239058e-06,
      "loss": 0.0315,
      "step": 2730
    },
    {
      "epoch": 2.7676767676767677,
      "grad_norm": 0.07025489211082458,
      "learning_rate": 3.888888888888889e-06,
      "loss": 0.0659,
      "step": 2740
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 0.004478944465517998,
      "learning_rate": 3.720538720538721e-06,
      "loss": 0.0004,
      "step": 2750
    },
    {
      "epoch": 2.787878787878788,
      "grad_norm": 5.561966896057129,
      "learning_rate": 3.5521885521885525e-06,
      "loss": 0.0215,
      "step": 2760
    },
    {
      "epoch": 2.797979797979798,
      "grad_norm": 0.005716877058148384,
      "learning_rate": 3.3838383838383844e-06,
      "loss": 0.0078,
      "step": 2770
    },
    {
      "epoch": 2.808080808080808,
      "grad_norm": 0.010671096853911877,
      "learning_rate": 3.2154882154882155e-06,
      "loss": 0.0521,
      "step": 2780
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 0.0035591397900134325,
      "learning_rate": 3.0471380471380474e-06,
      "loss": 0.0005,
      "step": 2790
    },
    {
      "epoch": 2.8282828282828283,
      "grad_norm": 0.043823834508657455,
      "learning_rate": 2.8787878787878793e-06,
      "loss": 0.0004,
      "step": 2800
    },
    {
      "epoch": 2.8383838383838382,
      "grad_norm": 0.006311338394880295,
      "learning_rate": 2.7104377104377103e-06,
      "loss": 0.0008,
      "step": 2810
    },
    {
      "epoch": 2.8484848484848486,
      "grad_norm": 0.00397644704207778,
      "learning_rate": 2.5420875420875422e-06,
      "loss": 0.0355,
      "step": 2820
    },
    {
      "epoch": 2.8585858585858586,
      "grad_norm": 0.09566056728363037,
      "learning_rate": 2.3737373737373737e-06,
      "loss": 0.0005,
      "step": 2830
    },
    {
      "epoch": 2.8686868686868685,
      "grad_norm": 0.006191011518239975,
      "learning_rate": 2.2053872053872056e-06,
      "loss": 0.0005,
      "step": 2840
    },
    {
      "epoch": 2.878787878787879,
      "grad_norm": 0.2658982276916504,
      "learning_rate": 2.0370370370370375e-06,
      "loss": 0.0027,
      "step": 2850
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.0072624036110937595,
      "learning_rate": 1.8686868686868688e-06,
      "loss": 0.0005,
      "step": 2860
    },
    {
      "epoch": 2.898989898989899,
      "grad_norm": 0.0039035328663885593,
      "learning_rate": 1.7003367003367005e-06,
      "loss": 0.0004,
      "step": 2870
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.008752391673624516,
      "learning_rate": 1.5319865319865321e-06,
      "loss": 0.0351,
      "step": 2880
    },
    {
      "epoch": 2.919191919191919,
      "grad_norm": 0.07273181527853012,
      "learning_rate": 1.3636363636363636e-06,
      "loss": 0.0344,
      "step": 2890
    },
    {
      "epoch": 2.929292929292929,
      "grad_norm": 0.004767535720020533,
      "learning_rate": 1.1952861952861953e-06,
      "loss": 0.0244,
      "step": 2900
    },
    {
      "epoch": 2.9393939393939394,
      "grad_norm": 0.003692569211125374,
      "learning_rate": 1.026936026936027e-06,
      "loss": 0.0025,
      "step": 2910
    },
    {
      "epoch": 2.9494949494949494,
      "grad_norm": 0.004048157017678022,
      "learning_rate": 8.585858585858586e-07,
      "loss": 0.0374,
      "step": 2920
    },
    {
      "epoch": 2.9595959595959593,
      "grad_norm": 0.0037876726128160954,
      "learning_rate": 6.902356902356903e-07,
      "loss": 0.0097,
      "step": 2930
    },
    {
      "epoch": 2.9696969696969697,
      "grad_norm": 0.003905748948454857,
      "learning_rate": 5.218855218855219e-07,
      "loss": 0.0005,
      "step": 2940
    },
    {
      "epoch": 2.9797979797979797,
      "grad_norm": 0.005372457671910524,
      "learning_rate": 3.535353535353536e-07,
      "loss": 0.0005,
      "step": 2950
    },
    {
      "epoch": 2.98989898989899,
      "grad_norm": 0.00395020330324769,
      "learning_rate": 1.851851851851852e-07,
      "loss": 0.0006,
      "step": 2960
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.004142614547163248,
      "learning_rate": 1.6835016835016835e-08,
      "loss": 0.0004,
      "step": 2970
    }
  ],
  "logging_steps": 10,
  "max_steps": 2970,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1573932158254848.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
