{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1890,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015873015873015872,
      "grad_norm": 2.497408866882324,
      "learning_rate": 4.976190476190477e-05,
      "loss": 2.456,
      "step": 10
    },
    {
      "epoch": 0.031746031746031744,
      "grad_norm": 3.867034673690796,
      "learning_rate": 4.9497354497354496e-05,
      "loss": 2.332,
      "step": 20
    },
    {
      "epoch": 0.047619047619047616,
      "grad_norm": 4.106087684631348,
      "learning_rate": 4.923280423280424e-05,
      "loss": 2.0958,
      "step": 30
    },
    {
      "epoch": 0.06349206349206349,
      "grad_norm": 4.445093631744385,
      "learning_rate": 4.896825396825397e-05,
      "loss": 1.7745,
      "step": 40
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 6.11029052734375,
      "learning_rate": 4.8703703703703704e-05,
      "loss": 1.5088,
      "step": 50
    },
    {
      "epoch": 0.09523809523809523,
      "grad_norm": 7.8503193855285645,
      "learning_rate": 4.843915343915344e-05,
      "loss": 1.3219,
      "step": 60
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 6.141728401184082,
      "learning_rate": 4.817460317460318e-05,
      "loss": 1.0306,
      "step": 70
    },
    {
      "epoch": 0.12698412698412698,
      "grad_norm": 5.91708517074585,
      "learning_rate": 4.791005291005291e-05,
      "loss": 0.8838,
      "step": 80
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 3.813659191131592,
      "learning_rate": 4.764550264550265e-05,
      "loss": 0.8068,
      "step": 90
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 3.3011655807495117,
      "learning_rate": 4.738095238095238e-05,
      "loss": 0.8222,
      "step": 100
    },
    {
      "epoch": 0.1746031746031746,
      "grad_norm": 5.385964393615723,
      "learning_rate": 4.711640211640212e-05,
      "loss": 0.6875,
      "step": 110
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.922276496887207,
      "learning_rate": 4.685185185185185e-05,
      "loss": 0.6088,
      "step": 120
    },
    {
      "epoch": 0.20634920634920634,
      "grad_norm": 5.8394951820373535,
      "learning_rate": 4.658730158730159e-05,
      "loss": 0.5926,
      "step": 130
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 5.497623443603516,
      "learning_rate": 4.632275132275132e-05,
      "loss": 0.5188,
      "step": 140
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 5.606062412261963,
      "learning_rate": 4.605820105820106e-05,
      "loss": 0.534,
      "step": 150
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 7.853214263916016,
      "learning_rate": 4.5793650793650795e-05,
      "loss": 0.5106,
      "step": 160
    },
    {
      "epoch": 0.2698412698412698,
      "grad_norm": 12.169827461242676,
      "learning_rate": 4.552910052910053e-05,
      "loss": 0.4673,
      "step": 170
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 4.6856584548950195,
      "learning_rate": 4.526455026455027e-05,
      "loss": 0.3331,
      "step": 180
    },
    {
      "epoch": 0.30158730158730157,
      "grad_norm": 4.86579704284668,
      "learning_rate": 4.5e-05,
      "loss": 0.4133,
      "step": 190
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 2.4860424995422363,
      "learning_rate": 4.473544973544974e-05,
      "loss": 0.4472,
      "step": 200
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 8.321184158325195,
      "learning_rate": 4.4470899470899475e-05,
      "loss": 0.4583,
      "step": 210
    },
    {
      "epoch": 0.3492063492063492,
      "grad_norm": 15.748229026794434,
      "learning_rate": 4.4206349206349204e-05,
      "loss": 0.3207,
      "step": 220
    },
    {
      "epoch": 0.36507936507936506,
      "grad_norm": 8.74420166015625,
      "learning_rate": 4.394179894179895e-05,
      "loss": 0.4044,
      "step": 230
    },
    {
      "epoch": 0.38095238095238093,
      "grad_norm": 9.406221389770508,
      "learning_rate": 4.3677248677248676e-05,
      "loss": 0.2787,
      "step": 240
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 10.487260818481445,
      "learning_rate": 4.341269841269842e-05,
      "loss": 0.2497,
      "step": 250
    },
    {
      "epoch": 0.4126984126984127,
      "grad_norm": 1.179118037223816,
      "learning_rate": 4.314814814814815e-05,
      "loss": 0.2817,
      "step": 260
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 3.779353618621826,
      "learning_rate": 4.2883597883597885e-05,
      "loss": 0.2402,
      "step": 270
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 11.380324363708496,
      "learning_rate": 4.261904761904762e-05,
      "loss": 0.323,
      "step": 280
    },
    {
      "epoch": 0.4603174603174603,
      "grad_norm": 11.698358535766602,
      "learning_rate": 4.235449735449736e-05,
      "loss": 0.3756,
      "step": 290
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 7.950473308563232,
      "learning_rate": 4.208994708994709e-05,
      "loss": 0.4258,
      "step": 300
    },
    {
      "epoch": 0.49206349206349204,
      "grad_norm": 10.047136306762695,
      "learning_rate": 4.182539682539683e-05,
      "loss": 0.3007,
      "step": 310
    },
    {
      "epoch": 0.5079365079365079,
      "grad_norm": 8.58102035522461,
      "learning_rate": 4.156084656084656e-05,
      "loss": 0.2255,
      "step": 320
    },
    {
      "epoch": 0.5238095238095238,
      "grad_norm": 15.79951000213623,
      "learning_rate": 4.12962962962963e-05,
      "loss": 0.2999,
      "step": 330
    },
    {
      "epoch": 0.5396825396825397,
      "grad_norm": 14.335479736328125,
      "learning_rate": 4.103174603174603e-05,
      "loss": 0.3491,
      "step": 340
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 2.4246840476989746,
      "learning_rate": 4.076719576719577e-05,
      "loss": 0.4079,
      "step": 350
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 9.399579048156738,
      "learning_rate": 4.05026455026455e-05,
      "loss": 0.2602,
      "step": 360
    },
    {
      "epoch": 0.5873015873015873,
      "grad_norm": 3.7288403511047363,
      "learning_rate": 4.023809523809524e-05,
      "loss": 0.316,
      "step": 370
    },
    {
      "epoch": 0.6031746031746031,
      "grad_norm": 0.37035003304481506,
      "learning_rate": 3.9973544973544975e-05,
      "loss": 0.2845,
      "step": 380
    },
    {
      "epoch": 0.6190476190476191,
      "grad_norm": 4.471505641937256,
      "learning_rate": 3.970899470899471e-05,
      "loss": 0.3255,
      "step": 390
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 11.813396453857422,
      "learning_rate": 3.944444444444445e-05,
      "loss": 0.1726,
      "step": 400
    },
    {
      "epoch": 0.6507936507936508,
      "grad_norm": 10.565126419067383,
      "learning_rate": 3.917989417989418e-05,
      "loss": 0.2544,
      "step": 410
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 6.447445869445801,
      "learning_rate": 3.891534391534392e-05,
      "loss": 0.2682,
      "step": 420
    },
    {
      "epoch": 0.6825396825396826,
      "grad_norm": 2.8084828853607178,
      "learning_rate": 3.8650793650793655e-05,
      "loss": 0.3423,
      "step": 430
    },
    {
      "epoch": 0.6984126984126984,
      "grad_norm": 0.5092664957046509,
      "learning_rate": 3.8386243386243385e-05,
      "loss": 0.2041,
      "step": 440
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 17.470062255859375,
      "learning_rate": 3.812169312169313e-05,
      "loss": 0.2643,
      "step": 450
    },
    {
      "epoch": 0.7301587301587301,
      "grad_norm": 11.501303672790527,
      "learning_rate": 3.785714285714286e-05,
      "loss": 0.3951,
      "step": 460
    },
    {
      "epoch": 0.746031746031746,
      "grad_norm": 0.47288787364959717,
      "learning_rate": 3.759259259259259e-05,
      "loss": 0.1699,
      "step": 470
    },
    {
      "epoch": 0.7619047619047619,
      "grad_norm": 4.86036491394043,
      "learning_rate": 3.732804232804233e-05,
      "loss": 0.2459,
      "step": 480
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 0.4270505905151367,
      "learning_rate": 3.7063492063492065e-05,
      "loss": 0.2327,
      "step": 490
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 9.749223709106445,
      "learning_rate": 3.67989417989418e-05,
      "loss": 0.2933,
      "step": 500
    },
    {
      "epoch": 0.8095238095238095,
      "grad_norm": 5.20520544052124,
      "learning_rate": 3.653439153439154e-05,
      "loss": 0.2058,
      "step": 510
    },
    {
      "epoch": 0.8253968253968254,
      "grad_norm": 8.261893272399902,
      "learning_rate": 3.626984126984127e-05,
      "loss": 0.2461,
      "step": 520
    },
    {
      "epoch": 0.8412698412698413,
      "grad_norm": 5.30229377746582,
      "learning_rate": 3.600529100529101e-05,
      "loss": 0.2453,
      "step": 530
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 6.6368489265441895,
      "learning_rate": 3.574074074074074e-05,
      "loss": 0.2206,
      "step": 540
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 6.318873882293701,
      "learning_rate": 3.547619047619048e-05,
      "loss": 0.2028,
      "step": 550
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 3.5913760662078857,
      "learning_rate": 3.521164021164021e-05,
      "loss": 0.2536,
      "step": 560
    },
    {
      "epoch": 0.9047619047619048,
      "grad_norm": 8.640997886657715,
      "learning_rate": 3.4947089947089954e-05,
      "loss": 0.2161,
      "step": 570
    },
    {
      "epoch": 0.9206349206349206,
      "grad_norm": 0.9504724144935608,
      "learning_rate": 3.468253968253968e-05,
      "loss": 0.1681,
      "step": 580
    },
    {
      "epoch": 0.9365079365079365,
      "grad_norm": 5.335127353668213,
      "learning_rate": 3.441798941798942e-05,
      "loss": 0.1178,
      "step": 590
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 6.484248161315918,
      "learning_rate": 3.4153439153439155e-05,
      "loss": 0.2536,
      "step": 600
    },
    {
      "epoch": 0.9682539682539683,
      "grad_norm": 0.2265627384185791,
      "learning_rate": 3.388888888888889e-05,
      "loss": 0.3138,
      "step": 610
    },
    {
      "epoch": 0.9841269841269841,
      "grad_norm": 16.8515625,
      "learning_rate": 3.362433862433863e-05,
      "loss": 0.2509,
      "step": 620
    },
    {
      "epoch": 1.0,
      "grad_norm": 11.265646934509277,
      "learning_rate": 3.335978835978836e-05,
      "loss": 0.2226,
      "step": 630
    },
    {
      "epoch": 1.0158730158730158,
      "grad_norm": 5.560768127441406,
      "learning_rate": 3.309523809523809e-05,
      "loss": 0.1309,
      "step": 640
    },
    {
      "epoch": 1.0317460317460316,
      "grad_norm": 2.3305299282073975,
      "learning_rate": 3.2830687830687836e-05,
      "loss": 0.073,
      "step": 650
    },
    {
      "epoch": 1.0476190476190477,
      "grad_norm": 0.46402353048324585,
      "learning_rate": 3.2566137566137565e-05,
      "loss": 0.1592,
      "step": 660
    },
    {
      "epoch": 1.0634920634920635,
      "grad_norm": 0.8938189148902893,
      "learning_rate": 3.230158730158731e-05,
      "loss": 0.1226,
      "step": 670
    },
    {
      "epoch": 1.0793650793650793,
      "grad_norm": 7.809844017028809,
      "learning_rate": 3.203703703703704e-05,
      "loss": 0.0911,
      "step": 680
    },
    {
      "epoch": 1.0952380952380953,
      "grad_norm": 0.10113058984279633,
      "learning_rate": 3.177248677248677e-05,
      "loss": 0.148,
      "step": 690
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.19559606909751892,
      "learning_rate": 3.150793650793651e-05,
      "loss": 0.1327,
      "step": 700
    },
    {
      "epoch": 1.126984126984127,
      "grad_norm": 0.8050828576087952,
      "learning_rate": 3.1243386243386245e-05,
      "loss": 0.1951,
      "step": 710
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 6.2264404296875,
      "learning_rate": 3.097883597883598e-05,
      "loss": 0.1577,
      "step": 720
    },
    {
      "epoch": 1.1587301587301586,
      "grad_norm": 0.2619193494319916,
      "learning_rate": 3.071428571428572e-05,
      "loss": 0.1945,
      "step": 730
    },
    {
      "epoch": 1.1746031746031746,
      "grad_norm": 8.676234245300293,
      "learning_rate": 3.044973544973545e-05,
      "loss": 0.2342,
      "step": 740
    },
    {
      "epoch": 1.1904761904761905,
      "grad_norm": 9.927608489990234,
      "learning_rate": 3.018518518518519e-05,
      "loss": 0.1626,
      "step": 750
    },
    {
      "epoch": 1.2063492063492063,
      "grad_norm": 0.5454114079475403,
      "learning_rate": 2.9920634920634922e-05,
      "loss": 0.1829,
      "step": 760
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 9.239614486694336,
      "learning_rate": 2.965608465608466e-05,
      "loss": 0.2126,
      "step": 770
    },
    {
      "epoch": 1.2380952380952381,
      "grad_norm": 0.22244466841220856,
      "learning_rate": 2.939153439153439e-05,
      "loss": 0.1976,
      "step": 780
    },
    {
      "epoch": 1.253968253968254,
      "grad_norm": 11.255456924438477,
      "learning_rate": 2.912698412698413e-05,
      "loss": 0.1829,
      "step": 790
    },
    {
      "epoch": 1.2698412698412698,
      "grad_norm": 3.798081159591675,
      "learning_rate": 2.8862433862433863e-05,
      "loss": 0.0963,
      "step": 800
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 9.081376075744629,
      "learning_rate": 2.8597883597883603e-05,
      "loss": 0.1093,
      "step": 810
    },
    {
      "epoch": 1.3015873015873016,
      "grad_norm": 4.648775100708008,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 0.1062,
      "step": 820
    },
    {
      "epoch": 1.3174603174603174,
      "grad_norm": 5.562363624572754,
      "learning_rate": 2.806878306878307e-05,
      "loss": 0.1648,
      "step": 830
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.8857225179672241,
      "learning_rate": 2.7804232804232804e-05,
      "loss": 0.163,
      "step": 840
    },
    {
      "epoch": 1.3492063492063493,
      "grad_norm": 0.05800854042172432,
      "learning_rate": 2.7539682539682544e-05,
      "loss": 0.1773,
      "step": 850
    },
    {
      "epoch": 1.3650793650793651,
      "grad_norm": 0.14854146540164948,
      "learning_rate": 2.7275132275132276e-05,
      "loss": 0.1015,
      "step": 860
    },
    {
      "epoch": 1.380952380952381,
      "grad_norm": 0.08684205263853073,
      "learning_rate": 2.7010582010582012e-05,
      "loss": 0.1526,
      "step": 870
    },
    {
      "epoch": 1.3968253968253967,
      "grad_norm": 11.406997680664062,
      "learning_rate": 2.6746031746031745e-05,
      "loss": 0.2036,
      "step": 880
    },
    {
      "epoch": 1.4126984126984126,
      "grad_norm": 0.08990123867988586,
      "learning_rate": 2.6481481481481485e-05,
      "loss": 0.1032,
      "step": 890
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 8.439504623413086,
      "learning_rate": 2.6216931216931217e-05,
      "loss": 0.0609,
      "step": 900
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 13.237654685974121,
      "learning_rate": 2.5952380952380957e-05,
      "loss": 0.0982,
      "step": 910
    },
    {
      "epoch": 1.4603174603174602,
      "grad_norm": 7.10713005065918,
      "learning_rate": 2.568783068783069e-05,
      "loss": 0.0731,
      "step": 920
    },
    {
      "epoch": 1.4761904761904763,
      "grad_norm": 13.421669960021973,
      "learning_rate": 2.5423280423280425e-05,
      "loss": 0.1019,
      "step": 930
    },
    {
      "epoch": 1.492063492063492,
      "grad_norm": 0.06010795012116432,
      "learning_rate": 2.5158730158730158e-05,
      "loss": 0.1224,
      "step": 940
    },
    {
      "epoch": 1.507936507936508,
      "grad_norm": 10.582318305969238,
      "learning_rate": 2.4894179894179894e-05,
      "loss": 0.2025,
      "step": 950
    },
    {
      "epoch": 1.5238095238095237,
      "grad_norm": 1.8966060876846313,
      "learning_rate": 2.462962962962963e-05,
      "loss": 0.133,
      "step": 960
    },
    {
      "epoch": 1.5396825396825395,
      "grad_norm": 0.45227745175361633,
      "learning_rate": 2.4365079365079366e-05,
      "loss": 0.177,
      "step": 970
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 12.157533645629883,
      "learning_rate": 2.4100529100529103e-05,
      "loss": 0.1791,
      "step": 980
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.2227252870798111,
      "learning_rate": 2.3835978835978835e-05,
      "loss": 0.1683,
      "step": 990
    },
    {
      "epoch": 1.5873015873015874,
      "grad_norm": 3.7299489974975586,
      "learning_rate": 2.357142857142857e-05,
      "loss": 0.1656,
      "step": 1000
    },
    {
      "epoch": 1.6031746031746033,
      "grad_norm": 0.3066585063934326,
      "learning_rate": 2.3306878306878307e-05,
      "loss": 0.1288,
      "step": 1010
    },
    {
      "epoch": 1.619047619047619,
      "grad_norm": 1.7325761318206787,
      "learning_rate": 2.3042328042328043e-05,
      "loss": 0.0938,
      "step": 1020
    },
    {
      "epoch": 1.6349206349206349,
      "grad_norm": 8.955907821655273,
      "learning_rate": 2.277777777777778e-05,
      "loss": 0.0619,
      "step": 1030
    },
    {
      "epoch": 1.6507936507936507,
      "grad_norm": 0.12353210896253586,
      "learning_rate": 2.2513227513227512e-05,
      "loss": 0.1823,
      "step": 1040
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 7.493529319763184,
      "learning_rate": 2.224867724867725e-05,
      "loss": 0.1317,
      "step": 1050
    },
    {
      "epoch": 1.6825396825396826,
      "grad_norm": 14.402583122253418,
      "learning_rate": 2.1984126984126984e-05,
      "loss": 0.1507,
      "step": 1060
    },
    {
      "epoch": 1.6984126984126984,
      "grad_norm": 0.07824938744306564,
      "learning_rate": 2.171957671957672e-05,
      "loss": 0.1205,
      "step": 1070
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 2.7399675846099854,
      "learning_rate": 2.1455026455026457e-05,
      "loss": 0.1821,
      "step": 1080
    },
    {
      "epoch": 1.7301587301587302,
      "grad_norm": 12.970391273498535,
      "learning_rate": 2.1190476190476193e-05,
      "loss": 0.1919,
      "step": 1090
    },
    {
      "epoch": 1.746031746031746,
      "grad_norm": 3.8187434673309326,
      "learning_rate": 2.0925925925925925e-05,
      "loss": 0.1648,
      "step": 1100
    },
    {
      "epoch": 1.7619047619047619,
      "grad_norm": 0.15087470412254333,
      "learning_rate": 2.066137566137566e-05,
      "loss": 0.0776,
      "step": 1110
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 9.026874542236328,
      "learning_rate": 2.0396825396825398e-05,
      "loss": 0.0851,
      "step": 1120
    },
    {
      "epoch": 1.7936507936507935,
      "grad_norm": 3.322685956954956,
      "learning_rate": 2.0132275132275134e-05,
      "loss": 0.0328,
      "step": 1130
    },
    {
      "epoch": 1.8095238095238095,
      "grad_norm": 15.936989784240723,
      "learning_rate": 1.986772486772487e-05,
      "loss": 0.1452,
      "step": 1140
    },
    {
      "epoch": 1.8253968253968254,
      "grad_norm": 5.052534103393555,
      "learning_rate": 1.9603174603174602e-05,
      "loss": 0.2017,
      "step": 1150
    },
    {
      "epoch": 1.8412698412698414,
      "grad_norm": 0.33349353075027466,
      "learning_rate": 1.933862433862434e-05,
      "loss": 0.112,
      "step": 1160
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 4.368480205535889,
      "learning_rate": 1.9074074074074075e-05,
      "loss": 0.2388,
      "step": 1170
    },
    {
      "epoch": 1.873015873015873,
      "grad_norm": 0.2504955530166626,
      "learning_rate": 1.880952380952381e-05,
      "loss": 0.0767,
      "step": 1180
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.04708895832300186,
      "learning_rate": 1.8544973544973547e-05,
      "loss": 0.1021,
      "step": 1190
    },
    {
      "epoch": 1.9047619047619047,
      "grad_norm": 1.2034810781478882,
      "learning_rate": 1.8280423280423283e-05,
      "loss": 0.1376,
      "step": 1200
    },
    {
      "epoch": 1.9206349206349205,
      "grad_norm": 0.3080185651779175,
      "learning_rate": 1.8015873015873015e-05,
      "loss": 0.0833,
      "step": 1210
    },
    {
      "epoch": 1.9365079365079365,
      "grad_norm": 18.01710319519043,
      "learning_rate": 1.775132275132275e-05,
      "loss": 0.154,
      "step": 1220
    },
    {
      "epoch": 1.9523809523809523,
      "grad_norm": 6.257868766784668,
      "learning_rate": 1.7486772486772488e-05,
      "loss": 0.0571,
      "step": 1230
    },
    {
      "epoch": 1.9682539682539684,
      "grad_norm": 3.1570518016815186,
      "learning_rate": 1.7222222222222224e-05,
      "loss": 0.11,
      "step": 1240
    },
    {
      "epoch": 1.9841269841269842,
      "grad_norm": 18.57176971435547,
      "learning_rate": 1.695767195767196e-05,
      "loss": 0.1347,
      "step": 1250
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.05456065014004707,
      "learning_rate": 1.6693121693121693e-05,
      "loss": 0.1229,
      "step": 1260
    },
    {
      "epoch": 2.015873015873016,
      "grad_norm": 0.10691910237073898,
      "learning_rate": 1.642857142857143e-05,
      "loss": 0.0907,
      "step": 1270
    },
    {
      "epoch": 2.0317460317460316,
      "grad_norm": 0.0904487818479538,
      "learning_rate": 1.6164021164021165e-05,
      "loss": 0.0482,
      "step": 1280
    },
    {
      "epoch": 2.0476190476190474,
      "grad_norm": 0.03990762680768967,
      "learning_rate": 1.58994708994709e-05,
      "loss": 0.1014,
      "step": 1290
    },
    {
      "epoch": 2.0634920634920633,
      "grad_norm": 0.4446806311607361,
      "learning_rate": 1.5634920634920637e-05,
      "loss": 0.1102,
      "step": 1300
    },
    {
      "epoch": 2.0793650793650795,
      "grad_norm": 4.51814603805542,
      "learning_rate": 1.537037037037037e-05,
      "loss": 0.0614,
      "step": 1310
    },
    {
      "epoch": 2.0952380952380953,
      "grad_norm": 0.6422652006149292,
      "learning_rate": 1.5105820105820107e-05,
      "loss": 0.0394,
      "step": 1320
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 1.1712599992752075,
      "learning_rate": 1.4841269841269842e-05,
      "loss": 0.057,
      "step": 1330
    },
    {
      "epoch": 2.126984126984127,
      "grad_norm": 0.045432668179273605,
      "learning_rate": 1.4576719576719578e-05,
      "loss": 0.0468,
      "step": 1340
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.06351105868816376,
      "learning_rate": 1.4312169312169312e-05,
      "loss": 0.0065,
      "step": 1350
    },
    {
      "epoch": 2.1587301587301586,
      "grad_norm": 3.083216667175293,
      "learning_rate": 1.4047619047619048e-05,
      "loss": 0.1127,
      "step": 1360
    },
    {
      "epoch": 2.1746031746031744,
      "grad_norm": 0.051810987293720245,
      "learning_rate": 1.3783068783068784e-05,
      "loss": 0.0489,
      "step": 1370
    },
    {
      "epoch": 2.1904761904761907,
      "grad_norm": 0.09818121790885925,
      "learning_rate": 1.3518518518518519e-05,
      "loss": 0.0484,
      "step": 1380
    },
    {
      "epoch": 2.2063492063492065,
      "grad_norm": 0.03822018578648567,
      "learning_rate": 1.3253968253968255e-05,
      "loss": 0.2253,
      "step": 1390
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.4393092393875122,
      "learning_rate": 1.2989417989417991e-05,
      "loss": 0.0782,
      "step": 1400
    },
    {
      "epoch": 2.238095238095238,
      "grad_norm": 0.5640565752983093,
      "learning_rate": 1.2724867724867725e-05,
      "loss": 0.0375,
      "step": 1410
    },
    {
      "epoch": 2.253968253968254,
      "grad_norm": 1.9727685451507568,
      "learning_rate": 1.2460317460317461e-05,
      "loss": 0.1136,
      "step": 1420
    },
    {
      "epoch": 2.2698412698412698,
      "grad_norm": 1.783286452293396,
      "learning_rate": 1.2195767195767196e-05,
      "loss": 0.0857,
      "step": 1430
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.01915910094976425,
      "learning_rate": 1.1931216931216932e-05,
      "loss": 0.1687,
      "step": 1440
    },
    {
      "epoch": 2.3015873015873014,
      "grad_norm": 0.3627309203147888,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 0.0535,
      "step": 1450
    },
    {
      "epoch": 2.317460317460317,
      "grad_norm": 1.3827433586120605,
      "learning_rate": 1.1402116402116402e-05,
      "loss": 0.0469,
      "step": 1460
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 0.03283467888832092,
      "learning_rate": 1.1137566137566138e-05,
      "loss": 0.0711,
      "step": 1470
    },
    {
      "epoch": 2.3492063492063493,
      "grad_norm": 5.865245342254639,
      "learning_rate": 1.0873015873015874e-05,
      "loss": 0.0634,
      "step": 1480
    },
    {
      "epoch": 2.365079365079365,
      "grad_norm": 0.19847708940505981,
      "learning_rate": 1.0608465608465609e-05,
      "loss": 0.0304,
      "step": 1490
    },
    {
      "epoch": 2.380952380952381,
      "grad_norm": 0.03447422757744789,
      "learning_rate": 1.0343915343915345e-05,
      "loss": 0.0468,
      "step": 1500
    },
    {
      "epoch": 2.3968253968253967,
      "grad_norm": 3.8054001331329346,
      "learning_rate": 1.007936507936508e-05,
      "loss": 0.0576,
      "step": 1510
    },
    {
      "epoch": 2.4126984126984126,
      "grad_norm": 8.050155639648438,
      "learning_rate": 9.814814814814815e-06,
      "loss": 0.0578,
      "step": 1520
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 15.453641891479492,
      "learning_rate": 9.550264550264551e-06,
      "loss": 0.2038,
      "step": 1530
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.02977285534143448,
      "learning_rate": 9.285714285714286e-06,
      "loss": 0.1341,
      "step": 1540
    },
    {
      "epoch": 2.4603174603174605,
      "grad_norm": 1.5706288814544678,
      "learning_rate": 9.021164021164022e-06,
      "loss": 0.1096,
      "step": 1550
    },
    {
      "epoch": 2.4761904761904763,
      "grad_norm": 0.09018097072839737,
      "learning_rate": 8.756613756613758e-06,
      "loss": 0.089,
      "step": 1560
    },
    {
      "epoch": 2.492063492063492,
      "grad_norm": 0.02896854281425476,
      "learning_rate": 8.492063492063492e-06,
      "loss": 0.0861,
      "step": 1570
    },
    {
      "epoch": 2.507936507936508,
      "grad_norm": 12.25561237335205,
      "learning_rate": 8.227513227513229e-06,
      "loss": 0.0873,
      "step": 1580
    },
    {
      "epoch": 2.5238095238095237,
      "grad_norm": 0.05024243891239166,
      "learning_rate": 7.962962962962963e-06,
      "loss": 0.0095,
      "step": 1590
    },
    {
      "epoch": 2.5396825396825395,
      "grad_norm": 0.07560574263334274,
      "learning_rate": 7.698412698412699e-06,
      "loss": 0.0086,
      "step": 1600
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 5.509312152862549,
      "learning_rate": 7.433862433862435e-06,
      "loss": 0.0578,
      "step": 1610
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 1.7136417627334595,
      "learning_rate": 7.16931216931217e-06,
      "loss": 0.1225,
      "step": 1620
    },
    {
      "epoch": 2.5873015873015874,
      "grad_norm": 0.033495090901851654,
      "learning_rate": 6.9047619047619055e-06,
      "loss": 0.0413,
      "step": 1630
    },
    {
      "epoch": 2.6031746031746033,
      "grad_norm": 1.25257408618927,
      "learning_rate": 6.640211640211641e-06,
      "loss": 0.0927,
      "step": 1640
    },
    {
      "epoch": 2.619047619047619,
      "grad_norm": 1.0532768964767456,
      "learning_rate": 6.375661375661377e-06,
      "loss": 0.0852,
      "step": 1650
    },
    {
      "epoch": 2.634920634920635,
      "grad_norm": 9.214005470275879,
      "learning_rate": 6.111111111111111e-06,
      "loss": 0.0555,
      "step": 1660
    },
    {
      "epoch": 2.6507936507936507,
      "grad_norm": 1.058599829673767,
      "learning_rate": 5.8465608465608465e-06,
      "loss": 0.0135,
      "step": 1670
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 6.3940887451171875,
      "learning_rate": 5.582010582010582e-06,
      "loss": 0.0209,
      "step": 1680
    },
    {
      "epoch": 2.682539682539683,
      "grad_norm": 0.47971516847610474,
      "learning_rate": 5.317460317460318e-06,
      "loss": 0.0978,
      "step": 1690
    },
    {
      "epoch": 2.6984126984126986,
      "grad_norm": 9.73294734954834,
      "learning_rate": 5.052910052910053e-06,
      "loss": 0.0498,
      "step": 1700
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 18.55547523498535,
      "learning_rate": 4.788359788359788e-06,
      "loss": 0.1845,
      "step": 1710
    },
    {
      "epoch": 2.7301587301587302,
      "grad_norm": 0.03331805020570755,
      "learning_rate": 4.5238095238095235e-06,
      "loss": 0.0699,
      "step": 1720
    },
    {
      "epoch": 2.746031746031746,
      "grad_norm": 0.7041895389556885,
      "learning_rate": 4.2592592592592596e-06,
      "loss": 0.0791,
      "step": 1730
    },
    {
      "epoch": 2.761904761904762,
      "grad_norm": 0.3526622951030731,
      "learning_rate": 3.994708994708995e-06,
      "loss": 0.0477,
      "step": 1740
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 4.239041805267334,
      "learning_rate": 3.7301587301587305e-06,
      "loss": 0.1883,
      "step": 1750
    },
    {
      "epoch": 2.7936507936507935,
      "grad_norm": 0.04176316410303116,
      "learning_rate": 3.4656084656084657e-06,
      "loss": 0.0473,
      "step": 1760
    },
    {
      "epoch": 2.8095238095238093,
      "grad_norm": 0.031479064375162125,
      "learning_rate": 3.2010582010582014e-06,
      "loss": 0.1097,
      "step": 1770
    },
    {
      "epoch": 2.825396825396825,
      "grad_norm": 0.049435049295425415,
      "learning_rate": 2.9365079365079366e-06,
      "loss": 0.0388,
      "step": 1780
    },
    {
      "epoch": 2.8412698412698414,
      "grad_norm": 0.054867882281541824,
      "learning_rate": 2.6719576719576723e-06,
      "loss": 0.056,
      "step": 1790
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 2.3459434509277344,
      "learning_rate": 2.4074074074074075e-06,
      "loss": 0.0205,
      "step": 1800
    },
    {
      "epoch": 2.873015873015873,
      "grad_norm": 0.025431480258703232,
      "learning_rate": 2.142857142857143e-06,
      "loss": 0.0213,
      "step": 1810
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 2.4352760314941406,
      "learning_rate": 1.8783068783068784e-06,
      "loss": 0.0722,
      "step": 1820
    },
    {
      "epoch": 2.9047619047619047,
      "grad_norm": 0.05697011202573776,
      "learning_rate": 1.6137566137566138e-06,
      "loss": 0.0735,
      "step": 1830
    },
    {
      "epoch": 2.9206349206349205,
      "grad_norm": 0.48253804445266724,
      "learning_rate": 1.3492063492063493e-06,
      "loss": 0.0213,
      "step": 1840
    },
    {
      "epoch": 2.9365079365079367,
      "grad_norm": 0.30351653695106506,
      "learning_rate": 1.0846560846560847e-06,
      "loss": 0.0254,
      "step": 1850
    },
    {
      "epoch": 2.9523809523809526,
      "grad_norm": 0.028889870271086693,
      "learning_rate": 8.201058201058201e-07,
      "loss": 0.0992,
      "step": 1860
    },
    {
      "epoch": 2.9682539682539684,
      "grad_norm": 0.01762939803302288,
      "learning_rate": 5.555555555555556e-07,
      "loss": 0.0409,
      "step": 1870
    },
    {
      "epoch": 2.984126984126984,
      "grad_norm": 2.366692543029785,
      "learning_rate": 2.91005291005291e-07,
      "loss": 0.084,
      "step": 1880
    },
    {
      "epoch": 3.0,
      "grad_norm": 36.125328063964844,
      "learning_rate": 2.645502645502646e-08,
      "loss": 0.0552,
      "step": 1890
    }
  ],
  "logging_steps": 10,
  "max_steps": 1890,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 187251218005248.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
